{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica-Bold;\f1\fswiss\fcharset0 Helvetica;\f2\fswiss\fcharset0 Helvetica-Oblique;
}
{\colortbl;\red255\green255\blue255;\red21\green21\blue22;\red109\green109\blue109;\red69\green73\blue76;
\red228\green234\blue244;}
{\*\expandedcolortbl;;\cssrgb\c10588\c10980\c11373;\cssrgb\c50196\c50196\c50196;\cssrgb\c34118\c35686\c37255;
\cssrgb\c91373\c93333\c96471;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{circle\}}{\leveltext\leveltemplateid2\'01\uc0\u9702 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{circle\}}{\leveltext\leveltemplateid102\'01\uc0\u9702 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid103\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid104\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid201\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{circle\}}{\leveltext\leveltemplateid202\'01\uc0\u9702 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid301\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{circle\}}{\leveltext\leveltemplateid302\'01\uc0\u9702 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid4}
{\list\listtemplateid5\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid401\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid402\'01\'01;}{\levelnumbers\'01;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid403\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid5}
{\list\listtemplateid6\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid501\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{circle\}}{\leveltext\leveltemplateid502\'01\uc0\u9702 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid6}
{\list\listtemplateid7\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid601\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{circle\}}{\leveltext\leveltemplateid602\'01\uc0\u9702 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid7}
{\list\listtemplateid8\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid701\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{circle\}}{\leveltext\leveltemplateid702\'01\uc0\u9702 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid8}
{\list\listtemplateid9\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid801\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{circle\}}{\leveltext\leveltemplateid802\'01\uc0\u9702 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid803\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid9}
{\list\listtemplateid10\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid901\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{circle\}}{\leveltext\leveltemplateid902\'01\uc0\u9702 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid10}
{\list\listtemplateid11\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1001\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid11}
{\list\listtemplateid12\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{circle\}}{\leveltext\leveltemplateid1102\'01\uc0\u9702 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid12}
{\list\listtemplateid13\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1201\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{circle\}}{\leveltext\leveltemplateid1202\'01\uc0\u9702 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid13}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}{\listoverride\listid5\listoverridecount0\ls5}{\listoverride\listid6\listoverridecount0\ls6}{\listoverride\listid7\listoverridecount0\ls7}{\listoverride\listid8\listoverridecount0\ls8}{\listoverride\listid9\listoverridecount0\ls9}{\listoverride\listid10\listoverridecount0\ls10}{\listoverride\listid11\listoverridecount0\ls11}{\listoverride\listid12\listoverridecount0\ls12}{\listoverride\listid13\listoverridecount0\ls13}}
\paperw11900\paperh16840\margl1440\margr1440\vieww16380\viewh12240\viewkind0
\deftab720
\pard\pardeftab720\sa160\partightenfactor0

\f0\b\fs28 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 \uc0\u55356 \u57307 \u65039  Unit I: Fundamentals of Machine Learning\
\pard\pardeftab720\sa320\partightenfactor0

\f1\b0 \cf2 \
\pard\pardeftab720\sa160\partightenfactor0

\f0\b \cf2 Core Concepts\
\pard\pardeftab720\sa160\partightenfactor0

\f1\b0 \cf2 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls1\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Traditional Programming vs. Machine Learning
\f1\b0 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls1\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Traditional Programming
\f1\b0 : You provide the computer with 
\f0\b Rules + Data
\f1\b0  to get 
\f0\b Answers
\f1\b0 . You manually code the logic.\
\ls1\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Machine Learning
\f1\b0 : You provide the computer with 
\f0\b Data + Answers
\f1\b0  so it can learn the 
\f0\b Rules
\f1\b0 . The algorithm infers the logic itself.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls1\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Key Components of an ML System
\f1\b0 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls1\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Data
\f1\b0 : The foundation for learning.\
\ls1\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Model
\f1\b0 : The mathematical representation that learns patterns from the data (e.g., a line in linear regression).\
\ls1\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Loss Function
\f1\b0 : A function that measures how "wrong" the model's predictions are compared to the actual values. The goal is to minimize this.\
\ls1\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	4	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Optimization Algorithm
\f1\b0 : The method used to adjust the model's parameters to minimize the loss function (e.g., Gradient Descent).\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa240\partightenfactor0
\ls1\ilvl0\cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Stages of a Machine Learning Pipeline\uc0\u8232 Data Ingestion \u10145 \u65039  Data Preprocessing \u10145 \u65039  Feature Engineering \u10145 \u65039  Model Training \u10145 \u65039  Model Evaluation \u10145 \u65039  Hyperparameter Tuning \u10145 \u65039  Deployment.\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\pard\pardeftab720\sa320\partightenfactor0
\cf2 \strokec2 \
\pard\pardeftab720\sa160\partightenfactor0

\f0\b \cf2 Mathematical Foundations \uc0\u55358 \u56800 \
\pard\pardeftab720\sa160\partightenfactor0

\f1\b0 \cf2 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls2\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Linear Algebra
\f1\b0 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls2\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Vector
\f1\b0 : A quantity with magnitude and direction, represented as a list of numbers. A single data sample is a feature vector. v=[v1 ,v2 ,...,vn ].\
\ls2\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Matrix
\f1\b0 : A rectangular array of numbers. A whole dataset is an m\'d7n matrix, where m is the number of samples and n is the number of features.\
\ls2\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Dot Product
\f1\b0 : A core operation in linear models. For vectors w and x, their dot product is w\uc0\u8901 x=wTx=\u8721 wi xi .\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls2\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Probability Theory
\f1\b0 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls2\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Conditional Probability
\f1\b0 : The probability of event A happening given that event B has occurred.\
\pard\tx1660\tx2160\pardeftab720\li2160\fi-2160\sa160\partightenfactor0
\ls2\ilvl2
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9642 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Formula
\f1\b0 : P(A\uc0\u8739 B)=P(B)P(A\u8745 B) \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls2\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Statistical Independence
\f1\b0 : Two events are independent if the occurrence of one does not affect the probability of the other. P(A\uc0\u8745 B)=P(A)P(B), which implies P(A\u8739 B)=P(A).\
\ls2\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Bayes' Theorem
\f1\b0 : Updates our prior belief about an event after new evidence is observed. It's the foundation for the Naive Bayes algorithm.\
\pard\tx1660\tx2160\pardeftab720\li2160\fi-2160\sa160\partightenfactor0
\ls2\ilvl2
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9642 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Formula
\f1\b0 : P(Class\uc0\u8739 Data)=P(Data)P(Data\u8739 Class)\u8901 P(Class) \
\pard\tx2380\tx2880\pardeftab720\li2880\fi-2880\sa160\partightenfactor0
\ls2\ilvl3\cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9642 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 P(Class\uc0\u8739 Data): 
\f0\b Posterior
\f1\b0  probability (what we want to find).\
\ls2\ilvl3\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9642 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 P(Data\uc0\u8739 Class): 
\f0\b Likelihood
\f1\b0  of observing the data given the class.\
\ls2\ilvl3\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9642 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 P(Class): 
\f0\b Prior
\f1\b0  probability of the class.\
\ls2\ilvl3\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9642 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 P(Data): 
\f0\b Evidence
\f1\b0 .\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls2\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Statistical Learning Concepts
\f1\b0 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls2\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Empirical Risk Minimization
\f1\b0 : The principle that we train a model by finding parameters that minimize the error on the training data.\
\ls2\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Generalization
\f1\b0 : The model's ability to perform well on new, unseen data. The difference between training error and test error is the 
\f0\b generalization gap
\f1\b0 .\
\ls2\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Inductive Bias
\f1\b0 : The set of assumptions a model makes to be able to generalize beyond the training data (e.g., linear regression's bias is that the underlying relationship between features and target is linear).\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls2\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Distributions & Functions
\f1\b0 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls2\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Probability Mass Function (PMF)
\f1\b0 : For 
\f0\b discrete
\f1\b0  random variables. Gives the probability of a specific outcome, P(X=x).\
\ls2\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Probability Density Function (PDF)
\f1\b0 : For 
\f0\b continuous
\f1\b0  random variables. The area under the curve between two points gives the probability of the variable falling in that interval.\
\pard\tx1660\tx2160\pardeftab720\li2160\fi-2160\sa160\partightenfactor0
\ls2\ilvl2
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9642 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 PDF for Normal Distribution
\f1\b0 : f(x\uc0\u8739 \u956 ,\u963 2)=2\u960 \u963 2{{\NeXTGraphic unknown.svg \width8000 \height21 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\tx1660\tx2160\pardeftab720\li2160\fi-2160\sa160\partightenfactor0
\ls2\ilvl2\cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9642 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 1 e\uc0\u8722 2\u963 2(x\u8722 \u956 )2 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls2\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Cumulative Distribution Function (CDF)
\f1\b0 : Gives the probability that a random variable X is less than or equal to a certain value x, i.e., F(x)=P(X\uc0\u8804 x).\
\ls2\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Expectation (Mean)
\f1\b0 : E[X]=\uc0\u956 . The long-run average value of a random variable.\
\pard\tx1660\tx2160\pardeftab720\li2160\fi-2160\sa160\partightenfactor0
\ls2\ilvl2\cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9642 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 For discrete RV: E[X]=\uc0\u8721 i xi P(xi )\
\ls2\ilvl2\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9642 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 For continuous RV: E[X]=\uc0\u8747 \u8722 \u8734 \u8734  xf(x)dx\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls2\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Variance
\f1\b0 : Var(X)=\uc0\u963 2=E[(X\u8722 \u956 )2]. Measures the average squared distance from the mean.\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\pard\pardeftab720\sa320\partightenfactor0
\cf2 \strokec2 \
\pard\pardeftab720\sa160\partightenfactor0

\f0\b \cf2 \uc0\u55357 \u57056 \u65039  Unit II: Data Handling & Feature Engineering\
\pard\pardeftab720\sa320\partightenfactor0

\f1\b0 \cf2 \
\
\pard\pardeftab720\sa160\partightenfactor0

\f0\b \cf2 Data Preprocessing\
\pard\pardeftab720\sa160\partightenfactor0

\f1\b0 \cf2 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls3\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Handling Missing Data
\f1\b0 :\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls3\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Deletion
\f1\b0 : Removing rows or columns with missing values (can lead to data loss).\
\ls3\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Imputation
\f1\b0 : Filling missing values with a substitute, like the 
\f0\b mean
\f1\b0 , 
\f0\b median
\f1\b0 , or 
\f0\b mode
\f1\b0  of the column.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls3\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Outlier Detection
\f1\b0 : Identifying data points that are unusually far from others.\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls3\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Z-score Method
\f1\b0 : Flag data points where the absolute Z-score is greater than a threshold (commonly 3). 
\f0\b Formula
\f1\b0 : Z=\uc0\u963 x\u8722 \u956  .\
\ls3\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 IQR Method
\f1\b0 : Flag points that fall below Q1\uc0\u8722 1.5\'d7IQR or above Q3+1.5\'d7IQR.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls3\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Categorical Data Encoding
\f1\b0 :\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls3\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Label Encoding
\f1\b0 : Assigns a unique integer to each category (e.g., \cf4 \cb5 \strokec4 ['cat', 'dog']\cf2 \cb1 \strokec2  \uc0\u8594  \cf4 \cb5 \strokec4 [0, 1]\cf2 \cb1 \strokec2 ). Best for 
\f0\b ordinal
\f1\b0  data where order matters.\
\ls3\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 One-Hot Encoding
\f1\b0 : Creates new binary columns for each category (e.g., \cf4 \cb5 \strokec4 cat\cf2 \cb1 \strokec2  \uc0\u8594  \cf4 \cb5 \strokec4 [1, 0]\cf2 \cb1 \strokec2 , \cf4 \cb5 \strokec4 dog\cf2 \cb1 \strokec2  \uc0\u8594  \cf4 \cb5 \strokec4 [0, 1]\cf2 \cb1 \strokec2 ). Best for 
\f0\b nominal
\f1\b0  data where order doesn't matter.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls3\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Feature Scaling
\f1\b0 : Putting features on a similar scale. This is essential for distance-based algorithms (KNN, SVM) and optimization algorithms like Gradient Descent.\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls3\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Min-Max Normalization
\f1\b0 : Scales data to a fixed range, usually [0, 1]. 
\f0\b Formula
\f1\b0 : Xnorm =Xmax \uc0\u8722 Xmin X\u8722 Xmin \
\ls3\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Standardization (Z-score)
\f1\b0 : Rescales data to have a mean of 0 and a standard deviation of 1. 
\f0\b Formula
\f1\b0 : Xstd =\uc0\u963 X\u8722 \u956  \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\pard\pardeftab720\sa320\partightenfactor0
\cf2 \strokec2 \
\pard\pardeftab720\sa160\partightenfactor0

\f0\b \cf2 Feature & Data Analysis\
\pard\pardeftab720\sa160\partightenfactor0

\f1\b0 \cf2 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls4\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Feature Selection
\f1\b0 : Choosing the most relevant features to improve model performance and reduce complexity.\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls4\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Filter Methods
\f1\b0 : Select features based on their statistical properties, independent of any model (e.g., 
\f0\b Pearson Correlation
\f1\b0 , 
\f0\b Chi-Squared test
\f1\b0 ). Fast.\
\ls4\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Wrapper Methods
\f1\b0 : Use the performance of a specific model to evaluate and select feature subsets (e.g., 
\f0\b Recursive Feature Elimination (RFE)
\f1\b0 ). Accurate but computationally expensive.\
\ls4\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Embedded Methods
\f1\b0 : Feature selection is an intrinsic part of the model training process (e.g., 
\f0\b L1 (Lasso) Regularization
\f1\b0  which can shrink feature weights to zero).\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls4\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Data Distributions
\f1\b0 : Understanding the shape of your data is key for EDA.\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls4\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Normal
\f1\b0 : Symmetrical bell curve.\
\ls4\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Skewed
\f1\b0 : An asymmetrical curve with a long tail on one side.\
\ls4\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Multimodal
\f1\b0 : A curve with two or more distinct peaks.\
\ls4\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Uniform
\f1\b0 : A flat shape where all outcomes are equally likely.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls4\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Sampling
\f1\b0 :\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls4\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Stratified Sampling
\f1\b0 : The data is divided into subgroups (strata), and samples are drawn from each stratum to preserve the original class proportions. 
\f0\b Crucial for imbalanced datasets
\f1\b0 .\
\ls4\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Bootstrapping
\f1\b0 : Randomly sampling data points from the dataset 
\f0\b with replacement
\f1\b0 . This is the core technique behind Bagging.\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\pard\pardeftab720\sa320\partightenfactor0
\cf2 \strokec2 \
\pard\pardeftab720\sa160\partightenfactor0

\f0\b \cf2 \uc0\u55357 \u56520  Unit III: Supervised Learning Algorithms\
\pard\pardeftab720\sa320\partightenfactor0

\f1\b0 \cf2 \
\
\pard\pardeftab720\sa160\partightenfactor0

\f0\b \cf2 Linear Regression\
\pard\pardeftab720\sa160\partightenfactor0

\f1\b0 \cf2 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls5\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Goal
\f1\b0 : Model the linear relationship between features and a continuous target variable.\
\ls5\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Hypothesis Function
\f1\b0 : hw (x)=wTx=w0 +w1 x1 +...+wn xn \
\ls5\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Cost Function (Mean Squared Error - MSE)
\f1\b0 : J(w)=2m1 \uc0\u8721 i=1m (hw (x(i))\u8722 y(i))2\
\ls5\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Optimization Methods
\f1\b0 :\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls5\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Gradient Descent (Iterative)
\f1\b0 : An algorithm that iteratively adjusts the weights w to find the minimum of the cost function.\
\pard\tx1660\tx2160\pardeftab720\li2160\fi-2160\sa160\partightenfactor0
\ls5\ilvl2
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9642 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Update Rule
\f1\b0 : wj :=wj \uc0\u8722 \u945 m1 \u8721 i=1m (hw (x(i))\u8722 y(i))xj(i) \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls5\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Normal Equation (Analytical)
\f1\b0 : A direct, one-step formula to solve for the optimal weights. It's fast for a small number of features but becomes slow (O(n3)) as features increase.\
\pard\tx1660\tx2160\pardeftab720\li2160\fi-2160\sa160\partightenfactor0
\ls5\ilvl2
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9642 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Formula
\f1\b0 : w=(XTX)\uc0\u8722 1XTy\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls5\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Assumptions (L.I.N.E.)
\f1\b0 : 
\f0\b L
\f1\b0 inearity, 
\f0\b I
\f1\b0 ndependence of errors, 
\f0\b N
\f1\b0 ormality of errors, 
\f0\b E
\f1\b0 qual variance of errors (Homoscedasticity).\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\pard\pardeftab720\sa320\partightenfactor0
\cf2 \strokec2 \
\pard\pardeftab720\sa160\partightenfactor0

\f0\b \cf2 Logistic Regression\
\pard\pardeftab720\sa160\partightenfactor0

\f1\b0 \cf2 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls6\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Goal
\f1\b0 : Predict a binary outcome (0 or 1) by modeling the probability of belonging to a class.\
\ls6\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Hypothesis Function
\f1\b0 : Uses the 
\f0\b Sigmoid (or Logistic) function
\f1\b0  to map any real-valued number into a value between 0 and 1.\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls6\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Formula
\f1\b0 : hw (x)=\uc0\u963 (wTx)=1+e\u8722 wTx1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls6\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Decision Boundary
\f1\b0 : The surface where the model's output probability is 0.5 (i.e., where wTx=0). The model predicts class 1 on one side and class 0 on the other.\
\ls6\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Cost Function (Log Loss / Binary Cross-Entropy)
\f1\b0 : J(w)=\uc0\u8722 m1 \u8721 i=1m [y(i)log(hw (x(i)))+(1\u8722 y(i))log(1\u8722 hw (x(i)))]\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\pard\pardeftab720\sa320\partightenfactor0
\cf2 \strokec2 \
\pard\pardeftab720\sa160\partightenfactor0

\f0\b \cf2 K-Nearest Neighbors (KNN)\
\pard\pardeftab720\sa160\partightenfactor0

\f1\b0 \cf2 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls7\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Concept
\f1\b0 : A simple, non-parametric, and instance-based algorithm. It classifies a new data point based on the majority vote of its \cf4 \cb5 \strokec4 k\cf2 \cb1 \strokec2  nearest neighbors.\
\ls7\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Key Points
\f1\b0 : It requires 
\f0\b feature scaling
\f1\b0  as it is distance-based.\
\ls7\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Pros
\f1\b0 : Simple to understand and implement. No training phase.\
\ls7\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Cons
\f1\b0 : Computationally expensive at prediction time, sensitive to irrelevant features, and performance degrades in high-dimensional spaces (the "Curse of Dimensionality").\
\ls7\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Distance Metrics
\f1\b0 :\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls7\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Euclidean Distance (L2 norm)
\f1\b0 : The straight-line distance. D(p,q)=\uc0\u8721 i=1n (pi \u8722 qi )2{{\NeXTGraphic 1__#$!@%!#__unknown.svg \width8000 \height25 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls7\ilvl1\cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls7\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Manhattan Distance (L1 norm)
\f1\b0 : The "city block" distance. D(p,q)=\uc0\u8721 i=1n \u8739 pi \u8722 qi \u8739 \
\ls7\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Minkowski Distance
\f1\b0 : The generalized form. D(p,q)=(\uc0\u8721 i=1n \u8739 pi \u8722 qi \u8739 p)1/p\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\pard\pardeftab720\sa320\partightenfactor0
\cf2 \strokec2 \
\pard\pardeftab720\sa160\partightenfactor0

\f0\b \cf2 Naive Bayes\
\pard\pardeftab720\sa160\partightenfactor0

\f1\b0 \cf2 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls8\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Concept
\f1\b0 : A family of probabilistic classifiers based on Bayes' Theorem with the "naive" assumption of 
\f0\b conditional independence
\f1\b0  between features (i.e., it assumes features are unrelated to each other, given the class).\
\ls8\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Classification Rule
\f1\b0 : Predict the class Ck that maximizes the posterior probability: y^ =kargmax \'a0P(Ck )\uc0\u8719 i=1n P(xi \u8739 Ck )\
\ls8\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Types
\f1\b0 :\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls8\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Gaussian NB
\f1\b0 : Assumes continuous features follow a normal distribution.\
\ls8\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Multinomial NB
\f1\b0 : Used for discrete count data (e.g., word counts for text classification).\
\ls8\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Bernoulli NB
\f1\b0 : Used for binary features (e.g., word is present or absent).\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\pard\pardeftab720\sa320\partightenfactor0
\cf2 \strokec2 \
\pard\pardeftab720\sa160\partightenfactor0

\f0\b \cf2 Support Vector Machines (SVM)\
\pard\pardeftab720\sa160\partightenfactor0

\f1\b0 \cf2 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls9\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Goal
\f1\b0 : To find the optimal hyperplane that best separates the classes by maximizing the 
\f0\b margin
\f1\b0  (the distance between the hyperplane and the nearest data points from either class).\
\ls9\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Support Vectors
\f1\b0 : These are the data points that lie closest to the hyperplane and on the margin. They are the critical elements that define the decision boundary.\
\ls9\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Hard vs. Soft Margin
\f1\b0 :\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls9\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Hard Margin
\f1\b0 : Used when the data is perfectly linearly separable. Objective is to minimize 21 \uc0\u8741 w\u8741 2 subject to yi (w\u8901 xi \u8722 b)\u8805 1.\
\ls9\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Soft Margin
\f1\b0 : Used for non-linearly separable data. It introduces a 
\f0\b slack variable
\f1\b0  \uc0\u958 i to allow for some misclassifications, penalizing them with a 
\f0\b Hinge Loss
\f1\b0  function.\
\pard\tx1660\tx2160\pardeftab720\li2160\fi-2160\sa160\partightenfactor0
\ls9\ilvl2
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9642 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Hinge Loss
\f1\b0 : L=max(0,1\uc0\u8722 yi (w\u8901 xi \u8722 b))\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls9\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 The Kernel Trick
\f1\b0 : For complex, non-linear data, SVMs use 
\f0\b kernels
\f1\b0  (e.g., Polynomial, RBF) to map the data into a higher-dimensional space where a linear hyperplane can be used to separate it.\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\pard\pardeftab720\sa320\partightenfactor0
\cf2 \strokec2 \
\pard\pardeftab720\sa160\partightenfactor0

\f0\b \cf2 Decision Trees\
\pard\pardeftab720\sa160\partightenfactor0

\f1\b0 \cf2 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls10\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Goal
\f1\b0 : Create a tree-like model where each internal node represents a test on a feature, each branch is an outcome of the test, and each leaf node is a class label.\
\ls10\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Splitting Criteria
\f1\b0 : Used to decide the best feature to split the data at each node.\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls10\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Gini Index
\f1\b0 : Measures the probability of misclassifying a randomly chosen element. Lower is better. 
\f0\b Formula
\f1\b0 : Gini=1\uc0\u8722 \u8721 i=1c (pi )2\
\ls10\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Entropy
\f1\b0 : Measures the level of impurity or randomness in a set. Lower is better. 
\f0\b Formula
\f1\b0 : H(S)=\uc0\u8722 \u8721 i=1c pi log2 (pi )\
\ls10\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Information Gain
\f1\b0 : Measures the reduction in entropy achieved by splitting on a feature. Higher is better. 
\f0\b Formula
\f1\b0 : IG(S,A)=H(S)\uc0\u8722 \u8721 v\u8712 Values(A) \u8739 S\u8739 \u8739 Sv \u8739  H(Sv )\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls10\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Regression Trees
\f1\b0 : For continuous targets, splits are made to achieve the greatest 
\f0\b Variance Reduction
\f1\b0 .\
\ls10\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Overfitting and Pruning
\f1\b0 : Trees can easily overfit by growing too deep. 
\f0\b Pruning
\f1\b0  is the process of removing branches to improve the tree's ability to generalize.\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\pard\pardeftab720\sa320\partightenfactor0
\cf2 \strokec2 \
\pard\pardeftab720\sa160\partightenfactor0

\f0\b \cf2 \uc0\u9989  Unit IV: Model Evaluation, Optimization, & Ensembles\
\pard\pardeftab720\sa320\partightenfactor0

\f1\b0 \cf2 \
\
\pard\pardeftab720\sa160\partightenfactor0

\f0\b \cf2 Classification Metrics\
\pard\pardeftab720\sa320\partightenfactor0

\f1\b0 \cf2 \
\pard\pardeftab720\sa160\partightenfactor0
\cf2 Metrics are derived from the 
\f0\b Confusion Matrix
\f1\b0 , which tabulates the performance of a classification model.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls11\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Accuracy
\f1\b0 : TP+TN+FP+FNTP+TN (Overall correctness; can be misleading for imbalanced datasets).\
\ls11\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Precision
\f1\b0 : TP+FPTP (Of all positive predictions, how many were actually positive? Measures exactness).\
\ls11\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Recall (Sensitivity, TPR)
\f1\b0 : TP+FNTP (Of all actual positives, how many did the model find? Measures completeness).\
\ls11\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Specificity (TNR)
\f1\b0 : TN+FPTN (Of all actual negatives, how many did the model find?).\
\ls11\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 F1-Score
\f1\b0 : 2\uc0\u8901 Precision+RecallPrecision\u8901 Recall (The harmonic mean of Precision and Recall; a great metric for imbalanced classes).\
\ls11\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 ROC Curve & AUC
\f1\b0 : The 
\f0\b Receiver Operating Characteristic
\f1\b0  curve plots 
\f0\b TPR
\f1\b0  vs. 
\f0\b FPR
\f1\b0  (FP/(FP+TN)) at various thresholds. The 
\f0\b Area Under the Curve (AUC)
\f1\b0  provides a single number summary of the model's performance across all thresholds.\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\pard\pardeftab720\sa320\partightenfactor0
\cf2 \strokec2 \
\pard\pardeftab720\sa160\partightenfactor0

\f0\b \cf2 Model Validation & Tuning\
\pard\pardeftab720\sa160\partightenfactor0

\f1\b0 \cf2 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls12\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Bias-Variance Trade-off
\f1\b0 : The central challenge in model fitting.\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls12\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 High Bias (Underfitting)
\f1\b0 : The model is too simple and makes strong assumptions, failing to capture the underlying patterns. High error on both train and test data.\
\ls12\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 High Variance (Overfitting)
\f1\b0 : The model is too complex and learns the noise in the training data. Low error on train data, but high error on test data.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls12\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Parameters vs. Hyperparameters
\f1\b0 :\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls12\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Parameters
\f1\b0 : Learned from the data by the model itself (e.g., the weights w in linear regression).\
\ls12\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Hyperparameters
\f1\b0 : Set by the user 
\f2\i before
\f1\i0  training to configure the model (e.g., the \cf4 \cb5 \strokec4 k\cf2 \cb1 \strokec2  in KNN, the learning rate \uc0\u945 ).\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls12\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Hyperparameter Tuning
\f1\b0 :\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls12\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Grid Search
\f1\b0 : Exhaustively tries every combination of a predefined set of hyperparameter values.\
\ls12\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Random Search
\f1\b0 : Randomly samples combinations of hyperparameter values. Often more efficient.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls12\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Cross-Validation
\f1\b0 :\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls12\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 k-Fold CV
\f1\b0 : A robust evaluation technique where data is split into 'k' folds; the model is trained on k-1 folds and tested on the remaining one, repeated k times.\
\ls12\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Stratified k-Fold CV
\f1\b0 : A variation for classification that preserves the class distribution within each fold. 
\f0\b Essential for imbalanced data
\f1\b0 .\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls12\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Regularization
\f1\b0 : Techniques to prevent overfitting by adding a penalty for model complexity to the cost function.\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls12\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 L1 Regularization (Lasso)
\f1\b0 : Adds a penalty of \uc0\u955 \u8721 \u8739 wj \u8739 . It can shrink some weights to exactly zero, performing implicit feature selection.\
\ls12\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 L2 Regularization (Ridge)
\f1\b0 : Adds a penalty of \uc0\u955 \u8721 wj2 . It pushes weights towards zero but does not set them to zero.\
\ls12\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Elastic Net
\f1\b0 : A combination of L1 and L2 penalties, useful when there are multiple correlated features. Adds penalty \uc0\u955 1 \u8721 \u8739 wj \u8739 +\u955 2 \u8721 wj2 .\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\pard\pardeftab720\sa320\partightenfactor0
\cf2 \strokec2 \
\pard\pardeftab720\sa160\partightenfactor0

\f0\b \cf2 Ensemble Learning\
\pard\pardeftab720\sa160\partightenfactor0

\f1\b0 \cf2 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls13\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Bagging (Bootstrap Aggregating)
\f1\b0 : Trains multiple models (e.g., Decision Trees) in parallel on different bootstrapped subsets of the data. The final prediction is an average or vote. 
\f0\b Reduces variance
\f1\b0 .\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls13\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Example
\f1\b0 : 
\f0\b Random Forest
\f1\b0 , which is an ensemble of Decision Trees.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls13\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Boosting
\f1\b0 : Trains models sequentially, where each new model is trained to correct the errors made by the previous ones. 
\f0\b Reduces bias
\f1\b0 .\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa160\partightenfactor0
\ls13\ilvl1
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 AdaBoost
\f1\b0 : Increases the weights of misclassified instances so the next model focuses more on them.\
\ls13\ilvl1
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Gradient Boosting
\f1\b0 : The next model is trained to predict the 
\f2\i residuals
\f1\i0  (errors) of the previous model. 
\f0\b Example
\f1\b0 : XGBoost.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls13\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Stacking
\f1\b0 : Trains several different base models and then uses another model (a "meta-learner") to learn how to best combine their predictions.\
}